{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2BP/ZUjads1GYjjYSvoPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/judeha/addnoise/blob/main/Add_noise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-dnmSMHXuxR",
        "outputId": "26f5be95-ed9f-4bf7-d768-3f03ddc018b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Drive setup\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "path=\"drive/My Drive/Dell Lab/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "M-Zh_dlBY0uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import datasets from hugging face\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "0AaV2hieZrnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "raw_datasets=load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "V-fC41OBZ29-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "conll=deepcopy(raw_datasets)"
      ],
      "metadata": {
        "id": "Q2yLBkm-ab15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import error data from drive | not necessary anymore \n",
        "fp = path + 'error_counts.npy'\n",
        "\n",
        "counts = np.load(fp)\n",
        "\n",
        "a_subs = counts[ord('a')]\n",
        "\n",
        "for i, x in enumerate(a_subs):\n",
        "    if x != 0:\n",
        "        print('{}: {}'.format(chr(i), x))"
      ],
      "metadata": {
        "id": "MzZ4CqjhapKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import homoglyph list | not necessary anymore\n",
        "f = open(path + \"homoglyph_list.json\")\n",
        "\n",
        "homoglyph = pd.read_json(f)\n",
        "homoglyph = homoglyph.sort_values(by=[\"a\",\"b\"])\n",
        "print(homoglyph)"
      ],
      "metadata": {
        "id": "KYNSjWabdN-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine tokens into text\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "corpus = conll['train']['tokens']\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "  corpus[i] = TreebankWordDetokenizer().detokenize(corpus[i])\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "AibT67Zcert1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and reshape homoglyph list\n",
        "lst = pd.read_csv(path + 'homoglyph.csv')\n",
        "lst = lst.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "df = pd.DataFrame(lst)\n",
        "homoglyph = pd.pivot_table(df, index='a', columns='b', values='sim_score', fill_value=0)\n",
        "\n",
        "print(homoglyph)"
      ],
      "metadata": {
        "id": "O8gf_260irR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate probability of substitution\n",
        "import statistics as stats\n",
        "from math import isnan\n",
        "\n",
        "err = 0.05\n",
        "\n",
        "# find sums of similarities \n",
        "sums = []\n",
        "\n",
        "# NOTE: not sure why iterrows doesn't work here\n",
        "for i in range(len(homoglyph.index)):\n",
        "  row = homoglyph.iloc[i]\n",
        "  lst = row.values.tolist()\n",
        "  lst = [x for x in lst if isnan(x)==False]\n",
        "  sums.append(sum(lst))\n",
        "\n",
        "print(sums)\n",
        "# standardize using max sum\n",
        "mx = max(sums)\n",
        "\n",
        "def helper(n):\n",
        "  return err * (n/mx)\n",
        "\n",
        "subst_prob = list(map(helper,sums))\n",
        "print(subst_prob)"
      ],
      "metadata": {
        "id": "g5YcQElzsI2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert back to dataframe with labels\n",
        "labels = homoglyph.columns.values.tolist()\n",
        "arr = [labels, subst_prob]\n",
        "\n",
        "ref = pd.DataFrame(arr, columns=labels)\n",
        "ref.fillna(0)\n",
        "print(ref)"
      ],
      "metadata": {
        "id": "B5kO9nc_wBii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize similarity scores to get CDFs\n",
        "def find_max(r):\n",
        "  return r.max(axis=0)\n",
        "\n",
        "def normalize(n, factor):\n",
        "  return n / factor\n",
        "\n",
        "homoglyph = homoglyph.apply(lambda row : normalize(row, find_max(row)), axis=1)\n",
        "print(homoglyph)"
      ],
      "metadata": {
        "id": "2iLpcJrox_gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset_selective -f \"homoglyph\""
      ],
      "metadata": {
        "id": "BkrU_c9K_0lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import S\n",
        "from prompt_toolkit.layout.dummy import D\n",
        "# Sort similarity scores by CDFs for each char\n",
        "# Construct dict of k,v pairs\n",
        "\n",
        "subst_cdf = {}\n",
        "for r in range(len(homoglyph.index)):\n",
        "  Dict = {}\n",
        "  for c in range(len(homoglyph.columns)):\n",
        "    Dict.update({homoglyph.iloc[r][c]: homoglyph.columns.values[c]})\n",
        "  s=dict(sorted(Dict.items()))\n",
        "  subst_cdf.update({homoglyph.index[r]: s})"
      ],
      "metadata": {
        "id": "hNb_chs6zBmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitution function\n",
        "def substitute(c: str):\n",
        "  indicator = ref[c]\n",
        "  bin = np.random.binomial(1, indicator)\n",
        "  if bin:\n",
        "    prob = np.random.uniform(0,1)\n",
        "    # NOTE: unfinished, need to index replacement from homoglyph.index(c) that is closest to prob. let this equal replacement\n",
        "    replacement = 'a'\n",
        "    return replacement\n",
        "  else: return c"
      ],
      "metadata": {
        "id": "IPdsm7Fh4GOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on colln\n",
        "for text in corpus[1:4]:\n",
        "  for i in range(len(text)):\n",
        "    temp = list(text)\n",
        "    print(temp)\n",
        "    # NOTE: conversion not working\n",
        "    temp[i] = substitute(temp[i])\n",
        "    text = \"\".join(temp)"
      ],
      "metadata": {
        "id": "KCtV7nt66YUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import json\n",
        "import random\n",
        "from bisect import bisect_left\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "def substitute(char):\n",
        "    if char in char_list and random.random() < scaled_sim_sums[char]:\n",
        "        i = bisect_left(subst_cdfs_probs[char], random.random())\n",
        "        return subst_cdfs_chars[char][i]\n",
        "    else:\n",
        "        return char\n",
        "\n",
        "\n",
        "def insert():\n",
        "    return random.choice(char_list)\n",
        "\n",
        "\n",
        "def add_noise_to_word(word):\n",
        "    chars = list(word)\n",
        "    for i, c in enumerate(chars):\n",
        "        if random.random() < 0.02:\n",
        "            chars[i] = ''\n",
        "            continue\n",
        "        if random.random() < 0.02:\n",
        "            chars[i] = substitute(c)\n",
        "        if random.random() < 0.02:\n",
        "            chars[i] = chars[i] + insert()\n",
        "        if random.random() < 0.02:\n",
        "            chars[i] = chars[i] + ' '\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def tokenize(string, label):\n",
        "    # Split on whitespace, keep only non-empty strings\n",
        "    words = list(filter(lambda w: w, string.split()))\n",
        "    if words == []:\n",
        "        return zip([], [])\n",
        "    # Associate labels with each word, changing beginning tags into\n",
        "    # inside tags for every word past the first\n",
        "    labels = [label] + [label + (label % 2) for _ in range(len(words)-1)]\n",
        "    return zip(words, labels)\n",
        "\n",
        "\n",
        "def add_noise_to_sentence(sentence):\n",
        "    # Randomly delete spaces\n",
        "    noisy_sentence1  = []\n",
        "    for word, label in sentence:\n",
        "        if random.random() < 0.02 and noisy_sentence1:\n",
        "            # Simluate space deletion by combining previous word with current word\n",
        "            prev_word, prev_label = noisy_sentence1[-1]\n",
        "            new_word = prev_word + word\n",
        "            new_label = prev_label if prev_label != 0 else label\n",
        "            noisy_sentence1[-1] = new_word, new_label\n",
        "        else:\n",
        "            noisy_sentence1.append((word, label))\n",
        "\n",
        "    # Add noise to each word\n",
        "    noisy_sentence2 = []\n",
        "    for word, label in noisy_sentence1:\n",
        "        noisy_string = add_noise_to_word(word)\n",
        "        # Tokenize the noisy string into words since the noise will add spaces\n",
        "        noisy_sentence2.extend(tokenize(noisy_string, label))\n",
        "\n",
        "    return noisy_sentence2\n",
        "\n",
        "\n",
        "def add_noise(data):\n",
        "    return [add_noise_to_sentence(sentence) for sentence in data]\n",
        "\n",
        "\n",
        "with open('homoglyph_probability_info.json') as f:\n",
        "    homoglyph_info = json.load(f)\n",
        "\n",
        "scaled_sim_sums = homoglyph_info['scaled_sim_sums']\n",
        "subst_cdfs = homoglyph_info['subst_cdfs']\n",
        "char_list = list(scaled_sim_sums.keys())\n",
        "\n",
        "subst_cdfs_chars = {\n",
        "    key_char: list(char_cum_probs.keys())\n",
        "    for key_char, char_cum_probs in subst_cdfs.items()\n",
        "}\n",
        "subst_cdfs_probs = {\n",
        "    key_char: list(char_cum_probs.values())\n",
        "    for key_char, char_cum_probs in subst_cdfs.items()\n",
        "}\n",
        "\n",
        "conll = datasets.load_dataset(\"conll2003\")\n",
        "conll_train, conll_val, conll_test = (\n",
        "    [\n",
        "        list(zip(sentence_words, sentence_labels))\n",
        "        for sentence_words, sentence_labels\n",
        "        in zip(conll[split]['tokens'], conll[split]['ner_tags'])\n",
        "    ]\n",
        "    for split in ('train', 'validation', 'test')\n",
        ")\n",
        "\n",
        "conll_train_noisy = add_noise(conll_train)\n",
        "conll_val_noisy = add_noise(conll_val)\n",
        "conll_test_noisy = add_noise(conll_test)\n",
        "\n",
        "id2label = {\n",
        "    0: 'O',\n",
        "    1: 'B-PER',\n",
        "    2: 'I-PER',\n",
        "    3: 'B-ORG',\n",
        "    4: 'I-ORG',\n",
        "    5: 'B-LOC',\n",
        "    6: 'I-LOC',\n",
        "    7: 'B-MISC',\n",
        "    8: 'I-MISC'\n",
        "}\n",
        "\n",
        "with open('noisy_conll-2003_train.txt', 'w') as f:\n",
        "    for sentence in conll_train_noisy:\n",
        "        for word, label in sentence:\n",
        "            f.write(f'{word} {id2label[label]}\\n')\n",
        "        f.write('\\n')\n",
        "with open('noisy_conll-2003_val.txt', 'w') as f:\n",
        "    for sentence in conll_val_noisy:\n",
        "        for word, label in sentence:\n",
        "            f.write(f'{word} {id2label[label]}\\n')\n",
        "        f.write('\\n')\n",
        "with open('noisy_conll-2003_test.txt', 'w') as f:\n",
        "    for sentence in conll_test_noisy:\n",
        "        for word, label in sentence:\n",
        "            f.write(f'{word} {id2label[label]}\\n')\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "CWDV7M0qw-IV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}